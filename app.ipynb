{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atLQGOMsb3Lb",
        "outputId": "864773d5-3d8e-4d88-ac68-67055c82c682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#importing required libraries\n",
        "import pandas as pd\n",
        "import requests as rqst\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    # Lowercase conversion\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removal of special characters and numbers\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation + string.digits))\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back to text\n",
        "    preprocessed_text = \" \".join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "    \n",
        "df = pd.read_csv('/content/amaze.csv')\n",
        "df['preprocessed_text'] = df['product_name'].apply(preprocess_text)\n",
        "df.to_csv('preprocessed_clothing_descriptions.csv', index=False)\n",
        "\n",
        "df = pd.read_csv(\"/content/preprocessed_clothing_descriptions.csv\")\n",
        "\n",
        "# Dump DataFrame to pickle\n",
        "with open(\"data.pkl\", \"wb\") as file:\n",
        "    pickle.dump(df, file)"
      ],
      "metadata": {
        "id": "MtBpI8MScZek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/preprocessed_clothing_descriptions.csv')\n",
        "\n",
        "# Load the clothing item descriptions from a CSV file\n",
        "df = pd.read_csv('/content/preprocessed_clothing_descriptions.csv')\n",
        "\n",
        "# Extract the descriptions from the 'description' column\n",
        "clothing_descriptions = df['preprocessed_text'].tolist()\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the clothing item descriptions\n",
        "vectorizer.fit(clothing_descriptions)\n",
        "\n",
        "\n",
        "\n",
        "pickle.dump(vector,open('tfidf1.pkl','wb'))\n",
        "model = pickle.load(open('tfidf1.pkl','rb'))\n",
        "\n"
      ],
      "metadata": {
        "id": "g-kotNwAgzWq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}